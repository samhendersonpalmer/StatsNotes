# (PART\*) Probability and inference {-} 

# Probability 

## Fundamental principles of probability

Statistical tests require probability for when we reject or fail to reject the null hypothesis.

Probability is just a way to measure how likely an event is to occur so it can be an observed proportion.

\begin{equation*}
Probability\: (Experimental)\:=\frac{number\: of\:times\:outcome\: occurs}{number\:of\:trials}
\end{equation*}

So if we flip a coin **once**, based on above what is the probability of heads $P(H)$? Well it can either be $P(H) = \frac{0}{1}$ or $P(H) = \frac{1}{1}$ because it either doesn't come up or it does.

But we don't actually always need observed data for probability, we can use theoretical method:

\begin{equation*}
Probability\: (Theoretical \:aka \:'a \:priori')\:=\frac{number\: of\:favourable\:outcomes}{number\:of\:outcomes\:in\:sample\:space}
\end{equation*}

'Favourable' just means outcomes that contain the event of interest and 'sample space' just means all possible outcomes.

So using the same example, if no coin has yet been flipped (as we're looking at this theoretically now) it helps us to think of the parameters in our equation. The number of favourable outcomes i.e. heads which is 1 and the number of outcomes in the sample space i.e. heads or tails which is 2. So $P(H) = \frac{1}{2}$.

Now if instead of our outcome being heads or not we might want it to be the number of heads in say 10 coin flips. So our sample space is now made up of all possible outcomes i.e. {0 heads, 1 heads, 2 heads...10 heads}. So probability of exactly 2 heads would be $P(2\:Heads) = \frac{1}{11}$ because including 0 heads there are 11 possible outcomes and only one of those is exactly 2 heads.

### Axioms

For a discrete sample space (S) (e.g. counts or categories) such as $S = \{Blonde, Brown, Ginger\}$ or $S = \{0, 1, 2\}$:

1. The probability of each outcome in the sample space has to be greater than or equal to 0
2. The sum of probabilities for all outcomes in sample space must equal 1
3. The probability of any event (a group of outcomes) is the sum of probabilities for all outcomes within it

Imagine A and B as two events (i.e. groups of possible outcomes) in sample space S. Event A could be "Male", and B could be "No job". Outcomes could then be {MY, MN, FY, FN}. Event A would cover those first two outcomes {MY, MN}.

**Intersection**: If outcomes are in both A and B $= A \cap B$. "A and B". If they don't have outcomes in common then this couldn't happen so they'd be called mutually exclusive.

**Union**: If the outcomes are in A, B or both $= A \cup B$. "A or B".

**Complement**: If the outcome is not in A $= A'$


### Addition law
This is the probability that either one or both events occur. The $- P(A \cap B)$ is so you're not counting the intersection twice (i.e. if A and B intersect then count the full circle of A which will also contain a bit of crossover from B).

\begin{equation*}
P(A \cup B) = P(A) + P(B) - P(A \cap B)
\end{equation*}

If A and B are mutually exclusive i.e. they can't occur together in the same trial then $P(A \cap B) = 0$ which leaves

\begin{eqnarray}
P(A \cup B) &=& P(A) + P(B) - 0 \\
P(A \cup B) &=& P(A) + P(B)
\end{eqnarray}


### Complementary rule
\begin{eqnarray}
P(A') &=& 1- P(A)
\end{eqnarray}

### Conditional probability
This is the probability of A given B i.e. we know B has occurred. The equation shows the "proportion of A's amongst the B's". Because the numerator is the intersection of A and B i.e. the shaded bit in common between both circles, so it's sort of like the part they have in common out of all of B.

\begin{eqnarray}
P(A | B) &=& \frac{P(A\cap B)}{P(B)}
\end{eqnarray}

### Multiplication law
This is the probability that both events occur. We can derive this from above:

\begin{eqnarray}
P(A\cap B) &=& P(A | B)P(B) &=& P(B | A)P(A)
\end{eqnarray}

Independent means $P(A|B) = P(A)$ i.e. it's not dependent on B so if independent:

\begin{eqnarray}
P(A\cap B) &=& P(A)P(B) \\
P(A\cap B \cap C) &=& P(A)P(B)P(C)
\end{eqnarray}

"Independent" just means knowing one event has occurred doesn't impact chances of other event occurring.

Requirement for independence (just need to verify one!):

1. $P(A|B) = P(A)$ or $P(B|A) = P(B)$
2. $P(A \cap B) = P(A)P(B)$


If events are **mutually exclusive** that means you have info that the other event can't happen so this can never be independent.



### Rule of total probability
This is the rule that says all probabilities should sum to 1 (if events are mutually exclusive and exhaustive).

e.g. Probability of detection (D) across three airports (A, B and C)

\begin{eqnarray}
P(D) &=& P(A \cap D) + P(B \cap D) + P(C \cap D) \\
\end{eqnarray}

Where $P(A \cap D) = P(D|A)P(A)$

### Bayes theorem
This is where we update info and can see how it changes estimates of probability. It allows us to flip conditional probabilities.

\begin{eqnarray}
P(A | B) &=& \frac{P(A\cap B)}{P(B)} \\
P(A\cap B) &=& P(A|B)P(B) &=& P(B|A)P(A) \\
P(A | B) &=& \frac{P(B|A)P(A)}{P(B)}
\end{eqnarray}

So $P(A|B)$ is called our posterior probability of A and the $P(A)$ is our prior probability of A.


## Random Variables
A random variable is just something that varies from person to another or even same person at different times of recording. Written as X, Y, Z etc (so x, y, z for actual specific values). X ~ just means X is distributed (i.e. explained) by.

Can be continuous (e.g. height) or discrete (e.g. job/no job). Therefore the distribution of these can also be discrete or continuous (all outcomes in the sample space). Can get proportions of answers to "Do you have a job?" from experimental or theoretical (creates using random variable and parameters).

### Discrete random variables
Here the sample space is countable.

#### Probability mass function
Probability mass function shows us the probability of each of those exact values occurring (pmf):
\begin{eqnarray}
P(X = x_i) = p(x_i) && \text{valid for all sum to 1}
\end{eqnarray}

So random variable X takes the value $x_i$ so $p(x_i)$ is probability of that specific value.

#### Cumulative distribution function
Each pmf has a cumulative distribution function (cdf) which is basically the probability of that value or lower i.e. it's cumulative probability:
\begin{eqnarray}
P(X = x_i) = F(x_i) = \displaystyle\sum_{r \le i} p(x_r)
\end{eqnarray}

So this just shows summing of all probabilities of all values of X for value $x_i$ and below.

### Continuous random variables
So with continuous random variables the sample space is theoretically infinite so any exact value has a probability of 0 i.e. $p(x_i) = 0$. This is more intuitive when you think of height which could be 1.8320390000000001 metres and the exact probability of that value is virtually 0 so instead we need to look at intervals to retrieve probability.

#### Probability density function
Now continuous random variables have a probability **density** function (pdf). 'Density' is the y-axis value and just measures the probability per unit value of the random variable. It describes the relative likelihood that a random variables takes a given value. Density can be greater than 1 but it's the integrated density of whole pdf curve that can't be (because that's the probability!). Valid if:

\begin{eqnarray}
f(x) \ge 0 && \text{for} && -\infty \le x \le \infty 
\end{eqnarray}

\begin{eqnarray}
\displaystyle\int^{\infty}_{-\infty} f(x) \: dx = 1 && \text{i.e. area under the pdf curve}
\end{eqnarray}

So because we're looking at intervals, we can calculate the area under the pdf by integrating (i.e. add 1 to the power then divide by the new power) to get the probability:

\begin{eqnarray}
P(a \le X \le b) &&=&& \displaystyle\int^{b}_{a} f(x) \: dx \\
P(X \le b) &&=&& \displaystyle\int^{b}_{-\infty} f(x) \: dx
\end{eqnarray}

Here we an actually drop the "equals" part of the $\le$ sign because remember any exact probability is 0 so we can instead just put $<$.

Example of what a pdf is usually written like:
\begin{eqnarray}
f(x) =
    \begin{cases}
      0.5(1-x) & \text{$-1 \le x \le 1$}\\
      0 & \text{otherwise}
    \end{cases}  
\end{eqnarray}

pdf integrates to 1 so we could use this to solve if instead of 0.5 it was k for example:
\begin{eqnarray}
\displaystyle\int^{1}_{-1} k(1-x)\: dx &&=&& 1 \\
k\displaystyle\int^{1}_{-1} (1-x)\: dx &&=&& 1 \\
k[x - \frac{x^2}{2}]^{1}_{-1} &&=&& 1
\end{eqnarray}


#### Cumulative distribution function
Each pdf also has an associated cdf:
\begin{eqnarray}
F(x) &&=&& \displaystyle\int^{x}_{-\infty} f(x) \: dx && \text{this is the same as} \: P(X < x) \\
F(x) &&=&& 0 && \text{if x is lower than lower limit} \\
F(x) &&=&& 1 && \text{if x is higher than the upper limit}
\end{eqnarray}

So $P(a < X < b)$ is area under pdf between a and b:
\begin{eqnarray}
\displaystyle\int^{b}_{a} f(x) \: dx = F(b) - F(a)
\end{eqnarray}

We can reverse this to get the pdf from cdf by differentiating:
\begin{eqnarray}
f(x) = \frac{dF(x)}{dx} && \text{aka multiply by power and subtract one from power}
\end{eqnarray}


## Summary statistics of random variables and their distributions

### Expectation
The expected value is the mean so it's basically a weighted average and purely theoretical, just used to describe the centre of the distribution.

For discrete it's given as $E[X] = \displaystyle\sum^{n}_{i = 1} x_ip(x_i)$.

For continuous random variables it's $E[X] = \displaystyle\int^{\infty}_{-\infty} xf(x) \: dx$.

### Moments and linear functions
These are descriptors of these random variable distributions, describing their location, scale and shape:

$k^{th}$ moment of random variable is $E[(X-b)^k]$ i.e. about a constant value of b


$0^{th}$ moment is always total mass/probability:  $E[(X-b^0)] = 1$


$1^{st}$ moment is always the mean:  $E[(X-0^1)] = E[X] = \mu$ 


$2^{nd}$ moment is always the variance about the mean (i.e. $b = \mu$):  $E[(X-\mu^2)] = var[X] = \sigma^2$ 



So variance $var[X]$ (the expected squared distance of X from the mean vs sd[X] which is typical distance of X from the mean) equals:
\begin{eqnarray}
&&=&& E[(X-\mu^2)] \\
&&=&& E[X^2 - 2X\mu +\mu^2] && \text{$\mu$ is a constant so comes out of brackets} \\
&&=&& E[X^2] - 2\mu E[X] +\mu^2 && \text{Remember $E[X] = \mu$}\\
&&=&& E[X^2] - 2\mu^2 +\mu^2 \\
&&=&& E[X^2] - \mu^2
\end{eqnarray}

Then we can find $E[X^2]$ with:
\begin{eqnarray}
\sum x^2p(x) && \text{for discrete} \\
\int x^2f(x) && \text{for continuous}
\end{eqnarray}

#### Linear functions of a random variable
The way to think of this is that most problems we encounter we have a relationship between a response variable and one or more independent variables. If any of the independent variables are random then the response variable will also be. This means the probability distribution and the moments of the dependent variable will also be related to the independent random variables.

So how do we actually estimate the probability distribution of the dependent variable?

Say for example we have $Y = aX + b$ if a and b are constants and summation, integration and expectation are all linear operators (i.e. can pull out constants to front and separate out terms by +/- etc):

\begin{eqnarray}
E[aX] &&=&& aE[X] \\
E[aX + b] &&=&& aE[X]+ b \\
E[aX + bY] &&=&& aE[X] + bE[Y]
\end{eqnarray}


But variance as a function is not a linear operator as it's squared distance from mean:

\begin{eqnarray}
Var[aX] &&=&& a^2Var[X] \\
Var[aX + b] &&=&& a^2Var[X] && \text{see below for why b disappears} \\
Var[aX + bY] &&=&& a^2Var[X] + b^2Var[Y]
\end{eqnarray}

In the second equation above, b disappears as it's a constant which does make sense because if b is added to X (our value in the probability distribution) then the actual variance is unchanged, it would just mean the entire distribution would shift by however much is added.

#### Examples
Say we had some data on the number of notifications per year per service which we'll call random variable X, described by the following pdf:


```{r var_table, echo = FALSE}
x_pdf <- data.frame(x_i = c(0, 1, 2, 3, "Total"),
          `p(x_i)` = c(0.8, 0.1, 0.07, 0.03, NA),
          `x_ip(x_i)` = c(0, 0.1, 0.14, 0.09, 0.33),
          `x_i^2` = c(0, 1, 4, 9, NA),
          `x_i^2p(x_i)` = c(0, 0.1, 0.28, 0.27, 0.65))

options(knitr.kable.NA = '')

x_pdf |> 
  knitr::kable(col.names = 
                 c("$x_i$",
                   "$p(x_i)$",
                   "$x_ip(x_i)$",
                   "$x_i^2$",
                   "$x_i^2p(x_i)$"))

```

So from this we can get the expected number of notifications per year (0.33), the variance $Var[X] = \sigma^2 = E[X^2] - \mu^2 = 0.65 - 0.33^2 = 0.54$ notifications per service per year (squared).

If there were 1500 services and the average cost of each notification was £850, how much is this going to cost us each year and what would the variance and sd of this cost be?

1. First, we need to create a new variable Y to represent total cost per year $Y = abX$ where a and b are constants $Y = 1500(850)X$ 
2. So $E[Y] = E[abX] = abE[X] = 1500(850)(0.33) = £420, 750$
3. $Var[Y] = Var[abX] = (ab)^2Var[X]=1500^2(850)^2(0.54) = £877,837,824,900^2$
4. $sd[Y] = \sqrt{Var[Y]} = £936,930$


## Probability distributions