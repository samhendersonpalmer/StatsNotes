[["index.html", "Everything I Know About Stats Chapter 1 About 1.1 Quick definitions 1.2 General programming tips 1.3 Quotes", " Everything I Know About Stats Sam Henderson-Palmer 2025-03-21 Chapter 1 About These are notes from various course, books and talks that I’m keeping a note of for future use 1.1 Quick definitions 95% confidence interval: We can say with 95% certainty/confidence that the true population parameter of interest lies within the interval upon repeated sampling of the population. Or Ben Goldacres definition (from page 60 of “It’s a bit more complicated than that”): “it means that if you repeatedly took samples of a hundred, then on 95 per cent of those attempts, the true [parameter] would lie somewhere between the upper and lower limits of the 95 per cent confidence intervals of yout samples.” 1.2 General programming tips 1.2.1 This book To preview a specific page to prevent building the whole thing just do bookdown::preview_chapter(\"01-intro.Rmd\", 'bookdown::gitbook') then open the corresponding html file within the _book/ folder. 1.2.2 Project structure - Example of this here README.md - Project overview, usage and dependencies setup.R - Install and loads required packages (called at the top of each script) utils.R - General utility functions (i.e. custom functions used within scripts repeatedly) datasets/ - Stores raw and cleaned data scripts/ - Holds processing, viz and analysis scripts plots/ - Saves generated images and plots tables/ - Store exported analysis tables fonts/ (if needed for custom viz) 1.3 Quotes “We are barraged, every day, with a vast quantity of numerical data, presented with absolute certainty and fetishistic precision. In reality, many of these numbers amount to nothing more than statistical noise, the gentle static fuzz of random variation and sampling error, making figures drift up and down, following no pattern at all, like the changing roll of a dice. This, I confidently predict, will never change” Ben Goldacre, pg 61 “It’s a bit more complicated than that” "],["probability.html", "Chapter 2 Probability 2.1 Fundamental principles of probability 2.2 Random Variables 2.3 Summary statistics of random variables and their distributions 2.4 Probability distributions", " Chapter 2 Probability 2.1 Fundamental principles of probability Statistical tests require probability for when we reject or fail to reject the null hypothesis. Probability is just a way to measure how likely an event is to occur so it can be an observed proportion. \\[\\begin{equation*} Probability\\: (Experimental)\\:=\\frac{number\\: of\\:times\\:outcome\\: occurs}{number\\:of\\:trials} \\end{equation*}\\] So if we flip a coin once, based on above what is the probability of heads \\(P(H)\\)? Well it can either be \\(P(H) = \\frac{0}{1}\\) or \\(P(H) = \\frac{1}{1}\\) because it either doesn’t come up or it does. But we don’t actually always need observed data for probability, we can use theoretical method: \\[\\begin{equation*} Probability\\: (Theoretical \\:aka \\:&#39;a \\:priori&#39;)\\:=\\frac{number\\: of\\:favourable\\:outcomes}{number\\:of\\:outcomes\\:in\\:sample\\:space} \\end{equation*}\\] ‘Favourable’ just means outcomes that contain the event of interest and ‘sample space’ just means all possible outcomes. So using the same example, if no coin has yet been flipped (as we’re looking at this theoretically now) it helps us to think of the parameters in our equation. The number of favourable outcomes i.e. heads which is 1 and the number of outcomes in the sample space i.e. heads or tails which is 2. So \\(P(H) = \\frac{1}{2}\\). Now if instead of our outcome being heads or not we might want it to be the number of heads in say 10 coin flips. So our sample space is now made up of all possible outcomes i.e. {0 heads, 1 heads, 2 heads…10 heads}. So probability of exactly 2 heads would be \\(P(2\\:Heads) = \\frac{1}{11}\\) because including 0 heads there are 11 possible outcomes and only one of those is exactly 2 heads. 2.1.1 Axioms For a discrete sample space (S) (e.g. counts or categories) such as \\(S = \\{Blonde, Brown, Ginger\\}\\) or \\(S = \\{0, 1, 2\\}\\): The probability of each outcome in the sample space has to be greater than or equal to 0 The sum of probabilities for all outcomes in sample space must equal 1 The probability of any event (a group of outcomes) is the sum of probabilities for all outcomes within it Imagine A and B as two events (i.e. groups of possible outcomes) in sample space S. Event A could be “Male”, and B could be “No job”. Outcomes could then be {MY, MN, FY, FN}. Event A would cover those first two outcomes {MY, MN}. Intersection: If outcomes are in both A and B \\(= A \\cap B\\). “A and B”. If they don’t have outcomes in common then this couldn’t happen so they’d be called mutually exclusive. Union: If the outcomes are in A, B or both \\(= A \\cup B\\). “A or B”. Complement: If the outcome is not in A \\(= A&#39;\\) 2.1.2 Addition law This is the probability that either one or both events occur. The \\(- P(A \\cap B)\\) is so you’re not counting the intersection twice (i.e. if A and B intersect then count the full circle of A which will also contain a bit of crossover from B). \\[\\begin{equation*} P(A \\cup B) = P(A) + P(B) - P(A \\cap B) \\end{equation*}\\] If A and B are mutually exclusive i.e. they can’t occur together in the same trial then \\(P(A \\cap B) = 0\\) which leaves \\[\\begin{eqnarray} P(A \\cup B) &amp;=&amp; P(A) + P(B) - 0 \\\\ P(A \\cup B) &amp;=&amp; P(A) + P(B) \\end{eqnarray}\\] 2.1.3 Complementary rule \\[\\begin{eqnarray} P(A&#39;) &amp;=&amp; 1- P(A) \\end{eqnarray}\\] 2.1.4 Conditional probability This is the probability of A given B i.e. we know B has occurred. The equation shows the “proportion of A’s amongst the B’s”. Because the numerator is the intersection of A and B i.e. the shaded bit in common between both circles, so it’s sort of like the part they have in common out of all of B. \\[\\begin{eqnarray} P(A | B) &amp;=&amp; \\frac{P(A\\cap B)}{P(B)} \\end{eqnarray}\\] 2.1.5 Multiplication law This is the probability that both events occur. We can derive this from above: \\[\\begin{eqnarray} P(A\\cap B) &amp;=&amp; P(A | B)P(B) &amp;=&amp; P(B | A)P(A) \\end{eqnarray}\\] Independent means \\(P(A|B) = P(A)\\) i.e. it’s not dependent on B so if independent: \\[\\begin{eqnarray} P(A\\cap B) &amp;=&amp; P(A)P(B) \\\\ P(A\\cap B \\cap C) &amp;=&amp; P(A)P(B)P(C) \\end{eqnarray}\\] “Independent” just means knowing one event has occurred doesn’t impact chances of other event occurring. Requirement for independence (just need to verify one!): \\(P(A|B) = P(A)\\) or \\(P(B|A) = P(B)\\) \\(P(A \\cap B) = P(A)P(B)\\) If events are mutually exclusive that means you have info that the other event can’t happen so this can never be independent. 2.1.6 Rule of total probability This is the rule that says all probabilities should sum to 1 (if events are mutually exclusive and exhaustive). e.g. Probability of detection (D) across three airports (A, B and C) \\[\\begin{eqnarray} P(D) &amp;=&amp; P(A \\cap D) + P(B \\cap D) + P(C \\cap D) \\\\ \\end{eqnarray}\\] Where \\(P(A \\cap D) = P(D|A)P(A)\\) 2.1.7 Bayes theorem This is where we update info and can see how it changes estimates of probability. It allows us to flip conditional probabilities. \\[\\begin{eqnarray} P(A | B) &amp;=&amp; \\frac{P(A\\cap B)}{P(B)} \\\\ P(A\\cap B) &amp;=&amp; P(A|B)P(B) &amp;=&amp; P(B|A)P(A) \\\\ P(A | B) &amp;=&amp; \\frac{P(B|A)P(A)}{P(B)} \\end{eqnarray}\\] So \\(P(A|B)\\) is called our posterior probability of A and the \\(P(A)\\) is our prior probability of A. 2.2 Random Variables A random variable is just something that varies from person to another or even same person at different times of recording. Written as X, Y, Z etc (so x, y, z for actual specific values). X ~ just means X is distributed (i.e. explained) by. Can be continuous (e.g. height) or discrete (e.g. job/no job). Therefore the distribution of these can also be discrete or continuous (all outcomes in the sample space). Can get proportions of answers to “Do you have a job?” from experimental or theoretical (creates using random variable and parameters). 2.2.1 Discrete random variables Here the sample space is countable. 2.2.1.1 Probability mass function Probability mass function shows us the probability of each of those exact values occurring (pmf): \\[\\begin{eqnarray} P(X = x_i) = p(x_i) &amp;&amp; \\text{valid for all sum to 1} \\end{eqnarray}\\] So random variable X takes the value \\(x_i\\) so \\(p(x_i)\\) is probability of that specific value. 2.2.1.2 Cumulative distribution function Each pmf has a cumulative distribution function (cdf) which is basically the probability of that value or lower i.e. it’s cumulative probability: \\[\\begin{eqnarray} P(X = x_i) = F(x_i) = \\displaystyle\\sum_{r \\le i} p(x_r) \\end{eqnarray}\\] So this just shows summing of all probabilities of all values of X for value \\(x_i\\) and below. 2.2.2 Continuous random variables So with continuous random variables the sample space is theoretically infinite so any exact value has a probability of 0 i.e. \\(p(x_i) = 0\\). This is more intuitive when you think of height which could be 1.8320390000000001 metres and the exact probability of that value is virtually 0 so instead we need to look at intervals to retrieve probability. 2.2.2.1 Probability density function Now continuous random variables have a probability density function (pdf). ‘Density’ is the y-axis value and just measures the probability per unit value of the random variable. It describes the relative likelihood that a random variables takes a given value. Density can be greater than 1 but it’s the integrated density of whole pdf curve that can’t be (because that’s the probability!). Valid if: \\[\\begin{eqnarray} f(x) \\ge 0 &amp;&amp; \\text{for} &amp;&amp; -\\infty \\le x \\le \\infty \\end{eqnarray}\\] \\[\\begin{eqnarray} \\displaystyle\\int^{\\infty}_{-\\infty} f(x) \\: dx = 1 &amp;&amp; \\text{i.e. area under the pdf curve} \\end{eqnarray}\\] So because we’re looking at intervals, we can calculate the area under the pdf by integrating (i.e. add 1 to the power then divide by the new power) to get the probability: \\[\\begin{eqnarray} P(a \\le X \\le b) &amp;&amp;=&amp;&amp; \\displaystyle\\int^{b}_{a} f(x) \\: dx \\\\ P(X \\le b) &amp;&amp;=&amp;&amp; \\displaystyle\\int^{b}_{-\\infty} f(x) \\: dx \\end{eqnarray}\\] Here we an actually drop the “equals” part of the \\(\\le\\) sign because remember any exact probability is 0 so we can instead just put \\(&lt;\\). Example of what a pdf is usually written like: \\[\\begin{eqnarray} f(x) = \\begin{cases} 0.5(1-x) &amp; \\text{$-1 \\le x \\le 1$}\\\\ 0 &amp; \\text{otherwise} \\end{cases} \\end{eqnarray}\\] pdf integrates to 1 so we could use this to solve if instead of 0.5 it was k for example: \\[\\begin{eqnarray} \\displaystyle\\int^{1}_{-1} k(1-x)\\: dx &amp;&amp;=&amp;&amp; 1 \\\\ k\\displaystyle\\int^{1}_{-1} (1-x)\\: dx &amp;&amp;=&amp;&amp; 1 \\\\ k[x - \\frac{x^2}{2}]^{1}_{-1} &amp;&amp;=&amp;&amp; 1 \\end{eqnarray}\\] 2.2.2.2 Cumulative distribution function Each pdf also has an associated cdf: \\[\\begin{eqnarray} F(x) &amp;&amp;=&amp;&amp; \\displaystyle\\int^{x}_{-\\infty} f(x) \\: dx &amp;&amp; \\text{this is the same as} \\: P(X &lt; x) \\\\ F(x) &amp;&amp;=&amp;&amp; 0 &amp;&amp; \\text{if x is lower than lower limit} \\\\ F(x) &amp;&amp;=&amp;&amp; 1 &amp;&amp; \\text{if x is higher than the upper limit} \\end{eqnarray}\\] So \\(P(a &lt; X &lt; b)\\) is area under pdf between a and b: \\[\\begin{eqnarray} \\displaystyle\\int^{b}_{a} f(x) \\: dx = F(b) - F(a) \\end{eqnarray}\\] We can reverse this to get the pdf from cdf by differentiating: \\[\\begin{eqnarray} f(x) = \\frac{dF(x)}{dx} &amp;&amp; \\text{aka multiply by power and subtract one from power} \\end{eqnarray}\\] 2.3 Summary statistics of random variables and their distributions 2.3.1 Expectation The expected value is the mean so it’s basically a weighted average and purely theoretical, just used to describe the centre of the distribution. For discrete it’s given as \\(E[X] = \\displaystyle\\sum^{n}_{i = 1} x_ip(x_i)\\). For continuous random variables it’s \\(E[X] = \\displaystyle\\int^{\\infty}_{-\\infty} xf(x) \\: dx\\). 2.3.2 Moments and linear functions These are descriptors of these random variable distributions, describing their location, scale and shape: \\(k^{th}\\) moment of random variable is \\(E[(X-b)^k]\\) i.e. about a constant value of b \\(0^{th}\\) moment is always total mass/probability: \\(E[(X-b^0)] = 1\\) \\(1^{st}\\) moment is always the mean: \\(E[(X-0^1)] = E[X] = \\mu\\) \\(2^{nd}\\) moment is always the variance about the mean (i.e. \\(b = \\mu\\)): \\(E[(X-\\mu^2)] = var[X] = \\sigma^2\\) So variance \\(var[X]\\) (the expected squared distance of X from the mean vs sd[X] which is typical distance of X from the mean) equals: \\[\\begin{eqnarray} &amp;&amp;=&amp;&amp; E[(X-\\mu^2)] \\\\ &amp;&amp;=&amp;&amp; E[X^2 - 2X\\mu +\\mu^2] &amp;&amp; \\text{$\\mu$ is a constant so comes out of brackets} \\\\ &amp;&amp;=&amp;&amp; E[X^2] - 2\\mu E[X] +\\mu^2 &amp;&amp; \\text{Remember $E[X] = \\mu$}\\\\ &amp;&amp;=&amp;&amp; E[X^2] - 2\\mu^2 +\\mu^2 \\\\ &amp;&amp;=&amp;&amp; E[X^2] - \\mu^2 \\end{eqnarray}\\] Then we can find \\(E[X^2]\\) with: \\[\\begin{eqnarray} \\sum x^2p(x) &amp;&amp; \\text{for discrete} \\\\ \\int x^2f(x) &amp;&amp; \\text{for continuous} \\end{eqnarray}\\] 2.3.2.1 Linear functions of a random variable The way to think of this is that most problems we encounter we have a relationship between a response variable and one or more independent variables. If any of the independent variables are random then the response variable will also be. This means the probability distribution and the moments of the dependent variable will also be related to the independent random variables. So how do we actually estimate the probability distribution of the dependent variable? Say for example we have \\(Y = aX + b\\) if a and b are constants and summation, integration and expectation are all linear operators (i.e. can pull out constants to front and separate out terms by +/- etc): \\[\\begin{eqnarray} E[aX] &amp;&amp;=&amp;&amp; aE[X] \\\\ E[aX + b] &amp;&amp;=&amp;&amp; aE[X]+ b \\\\ E[aX + bY] &amp;&amp;=&amp;&amp; aE[X] + bE[Y] \\end{eqnarray}\\] But variance as a function is not a linear operator as it’s squared distance from mean: \\[\\begin{eqnarray} Var[aX] &amp;&amp;=&amp;&amp; a^2Var[X] \\\\ Var[aX + b] &amp;&amp;=&amp;&amp; a^2Var[X] &amp;&amp; \\text{see below for why b disappears} \\\\ Var[aX + bY] &amp;&amp;=&amp;&amp; a^2Var[X] + b^2Var[Y] \\end{eqnarray}\\] In the second equation above, b disappears as it’s a constant which does make sense because if b is added to X (our value in the probability distribution) then the actual variance is unchanged, it would just mean the entire distribution would shift by however much is added. 2.3.2.2 Examples Say we had some data on the number of notifications per year per service which we’ll call random variable X, described by the following pdf: \\(x_i\\) \\(p(x_i)\\) \\(x_ip(x_i)\\) \\(x_i^2\\) \\(x_i^2p(x_i)\\) 0 0.80 0.00 0 0.00 1 0.10 0.10 1 0.10 2 0.07 0.14 4 0.28 3 0.03 0.09 9 0.27 Total 0.33 0.65 So from this we can get the expected number of notifications per year (0.33), the variance \\(Var[X] = \\sigma^2 = E[X^2] - \\mu^2 = 0.65 - 0.33^2 = 0.54\\) notifications per service per year (squared). If there were 1500 services and the average cost of each notification was £850, how much is this going to cost us each year and what would the variance and sd of this cost be? First, we need to create a new variable Y to represent total cost per year \\(Y = abX\\) where a and b are constants \\(Y = 1500(850)X\\) So \\(E[Y] = E[abX] = abE[X] = 1500(850)(0.33) = £420, 750\\) \\(Var[Y] = Var[abX] = (ab)^2Var[X]=1500^2(850)^2(0.54) = £877,837,824,900^2\\) \\(sd[Y] = \\sqrt{Var[Y]} = £936,930\\) 2.4 Probability distributions "],["inference.html", "Chapter 3 Inference 3.1 Hypothesis tests 3.2 Confidence intervals 3.3 Regression models", " Chapter 3 Inference 3.1 Hypothesis tests 3.2 Confidence intervals 3.3 Regression models "],["parts.html", "Chapter 4 Parts", " Chapter 4 Parts You can add parts to organize one or more book chapters together. Parts can be inserted at the top of an .Rmd file, before the first-level chapter heading in that same file. Add a numbered part: # (PART) Act one {-} (followed by # A chapter) Add an unnumbered part: # (PART\\*) Act one {-} (followed by # A chapter) Add an appendix as a special kind of un-numbered part: # (APPENDIX) Other stuff {-} (followed by # A chapter). Chapters in an appendix are prepended with letters instead of numbers. "],["footnotes-and-citations.html", "Chapter 5 Footnotes and citations 5.1 Footnotes 5.2 Citations", " Chapter 5 Footnotes and citations 5.1 Footnotes Footnotes are put inside the square brackets after a caret ^[]. Like this one 1. 5.2 Citations Reference items in your bibliography file(s) using @key. For example, we are using the bookdown package (Xie 2025) (check out the last code chunk in index.Rmd to see how this citation key was added) in this sample book, which was built on top of R Markdown and knitr (Xie 2015) (this citation was added manually in an external file book.bib). Note that the .bib files need to be listed in the index.Rmd with the YAML bibliography key. The RStudio Visual Markdown Editor can also make it easier to insert citations: https://rstudio.github.io/visual-markdown-editing/#/citations References Xie, Yihui. 2015. Dynamic Documents with R and Knitr. 2nd ed. Boca Raton, Florida: Chapman; Hall/CRC. http://yihui.org/knitr/. ———. 2025. Bookdown: Authoring Books and Technical Documents with r Markdown. https://github.com/rstudio/bookdown. This is a footnote.↩︎ "],["blocks.html", "Chapter 6 Blocks 6.1 Equations 6.2 Theorems and proofs 6.3 Callout blocks", " Chapter 6 Blocks 6.1 Equations Here is an equation. \\[\\begin{equation} f\\left(k\\right) = \\binom{n}{k} p^k\\left(1-p\\right)^{n-k} \\tag{6.1} \\end{equation}\\] You may refer to using \\@ref(eq:binom), like see Equation (6.1). 6.2 Theorems and proofs Labeled theorems can be referenced in text using \\@ref(thm:tri), for example, check out this smart theorem 6.1. Theorem 6.1 For a right triangle, if \\(c\\) denotes the length of the hypotenuse and \\(a\\) and \\(b\\) denote the lengths of the other two sides, we have \\[a^2 + b^2 = c^2\\] Read more here https://bookdown.org/yihui/bookdown/markdown-extensions-by-bookdown.html. 6.3 Callout blocks The R Markdown Cookbook provides more help on how to use custom blocks to design your own callouts: https://bookdown.org/yihui/rmarkdown-cookbook/custom-blocks.html "],["sharing-your-book.html", "Chapter 7 Sharing your book 7.1 Publishing 7.2 404 pages 7.3 Metadata for sharing", " Chapter 7 Sharing your book 7.1 Publishing HTML books can be published online, see: https://bookdown.org/yihui/bookdown/publishing.html 7.2 404 pages By default, users will be directed to a 404 page if they try to access a webpage that cannot be found. If you’d like to customize your 404 page instead of using the default, you may add either a _404.Rmd or _404.md file to your project root and use code and/or Markdown syntax. 7.3 Metadata for sharing Bookdown HTML books will provide HTML metadata for social sharing on platforms like Twitter, Facebook, and LinkedIn, using information you provide in the index.Rmd YAML. To setup, set the url for your book and the path to your cover-image file. Your book’s title and description are also used. This gitbook uses the same social sharing data across all chapters in your book- all links shared will look the same. Specify your book’s source repository on GitHub using the edit key under the configuration options in the _output.yml file, which allows users to suggest an edit by linking to a chapter’s source file. Read more about the features of this output format here: https://pkgs.rstudio.com/bookdown/reference/gitbook.html Or use: ?bookdown::gitbook "],["references.html", "References", " References Xie, Yihui. 2015. Dynamic Documents with R and Knitr. 2nd ed. Boca Raton, Florida: Chapman; Hall/CRC. http://yihui.org/knitr/. ———. 2025. Bookdown: Authoring Books and Technical Documents with r Markdown. https://github.com/rstudio/bookdown. "],["404.html", "Page not found", " Page not found The page you requested cannot be found (perhaps it was moved or renamed). You may want to try searching to find the page's new location, or use the table of contents to find the page you are looking for. "]]
